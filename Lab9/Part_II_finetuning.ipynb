{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install peft datasets"
      ],
      "metadata": {
        "id": "ejKaIQ6-mZww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UPoTcA7Nl2je"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
        "from peft import get_peft_model, LoraConfig"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_model_name = \"EleutherAI/pythia-1.4b\"\n",
        "model_max_length = 128\n",
        "\n",
        "# Low-Rank Adaptation (LoRA) is an adapters method for parameter\n",
        "# efficient finetuning. For more details, look at the paper:\n",
        "# https://arxiv.org/abs/2106.09685\n",
        "peft_config = LoraConfig(\n",
        "    r=32,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    # We apply LoRA to the attention matrices, as recommended by\n",
        "    # the paper authors.\n",
        "    target_modules=[\"query_key_value\", \"dense\"],\n",
        ")\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"output\",\n",
        "    # This is the batch size for each forward in the gpu.\n",
        "    per_device_train_batch_size=16,\n",
        "    # Gradient accumulation only performs the backward pass every\n",
        "    # n steps, accumulating gradients in between. This means that\n",
        "    # the effective batch size we are using is 16 * 2 = 32.\n",
        "    gradient_accumulation_steps=2,\n",
        "    learning_rate=2e-4,\n",
        "    # LR scheduler gradually reduces the learning rate from its\n",
        "    # initial value to 0 and usually leads to better results.\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    # Training for 1 epoch would be ideal but for time reasons\n",
        "    # we only do 400 steps.\n",
        "    # Uncomment the num_training_epochs and comment max_steps to see\n",
        "    # how the model performance changes with more data.\n",
        "    max_steps=400,\n",
        "    # num_train_epochs=1,\n",
        "    logging_steps=1,\n",
        "    # Enables mixed precision training. This performs the forward and\n",
        "    # backward computations in floating point with 16 bits. This leads\n",
        "    # to faster training due to specialized hardware instructions.\n",
        "    # For more information read the following paper:\n",
        "    # https://arxiv.org/abs/1710.03740\n",
        "    fp16=True,\n",
        ")"
      ],
      "metadata": {
        "id": "0cmPLID5mHkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    base_model_name, bos_token=\"<|startoftext|>\", pad_token=\"<|padding|>\",\n",
        ")\n",
        "tokenizer.max_length = model_max_length\n",
        "\n",
        "tokenizer"
      ],
      "metadata": {
        "id": "m36ulLZ-V2a6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(base_model_name, device_map=\"cuda\")\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "model = get_peft_model(model, peft_config)\n",
        "model.print_trainable_parameters()\n",
        "model"
      ],
      "metadata": {
        "id": "yxYkM3b4nDgO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The alpaca dataset is an instruction tuning dataset composed of the following features:\n",
        "# * instruction: the instruction for the model\n",
        "# * input: optional extra input (such as a text to summarize)\n",
        "# * output: text to be generated by the model\n",
        "# For more information look at: https://crfm.stanford.edu/2023/03/13/alpaca.html\n",
        "dataset = load_dataset(\"tatsu-lab/alpaca\")\n",
        "dataset"
      ],
      "metadata": {
        "id": "Y6YzjUMOqxVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = dataset[\"train\"]\n",
        "train"
      ],
      "metadata": {
        "id": "owQBRFjmQ-5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_prompt(instruction, input=None):\n",
        "  if input is None or len(input) == 0:\n",
        "    return f\"Instruction:\\n{instruction}\\nAnswer:\\n\"\n",
        "  return f\"Instruction:\\n{instruction}\\nInput:{input}\\n{input}\\nAnswer:\\n\"\n",
        "\n",
        "def tokenize_text(record):\n",
        "  instruction = record[\"instruction\"].strip()\n",
        "  input = record[\"input\"].strip()\n",
        "  prompt = create_prompt(instruction, input)\n",
        "  target = record[\"output\"].strip()\n",
        "  text = f\"{prompt}{target}\"\n",
        "  input_ids = tokenizer(text)[\"input_ids\"]\n",
        "  # Add bos and eos\n",
        "  input_ids = [tokenizer.bos_token_id] + input_ids + [tokenizer.eos_token_id]\n",
        "\n",
        "  labels = [t for t in input_ids]\n",
        "  return {\"input_ids\": input_ids, \"labels\": labels}\n",
        "\n",
        "train = train.map(tokenize_text)\n",
        "train"
      ],
      "metadata": {
        "id": "xSK0oP5MriWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Here, we could alternatively apply truncation and keep the first tokens of\n",
        "# the text until the model length is filled. However, since we have many records\n",
        "# we choose to discard the larger ones which will lead to incomplete texts.\n",
        "train = train.filter(lambda x: len(x[\"input_ids\"]) <= model_max_length)\n",
        "train"
      ],
      "metadata": {
        "id": "HjdMwgvRUvz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_to_max_length(record):\n",
        "  pad_len = model_max_length - len(record[\"input_ids\"])\n",
        "  record[\"input_ids\"] = record[\"input_ids\"] + [tokenizer.pad_token_id] * pad_len\n",
        "  # In the labels, we pad with -100 as this indicates to the cross entropy loss\n",
        "  # these entries should be ignored.\n",
        "  record[\"labels\"] = record[\"labels\"] + [-100] * pad_len\n",
        "  assert len(record[\"input_ids\"]) == model_max_length\n",
        "  return record\n",
        "\n",
        "train = train.map(pad_to_max_length)"
      ],
      "metadata": {
        "id": "TvrIo3FehM4q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for record in train.select(range(1)):\n",
        "  print(record[\"input_ids\"])\n",
        "  print(record[\"labels\"])\n",
        "  print(tokenizer.batch_decode(record[\"input_ids\"], skip_special_tokens=False))\n",
        "  print()\n"
      ],
      "metadata": {
        "id": "-8U_C9YIsrdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad\n",
        "def run_instruction(instruction, model):\n",
        "  prompt = create_prompt(instruction)\n",
        "  input_ids = tokenizer(prompt)[\"input_ids\"]\n",
        "  input_ids = [tokenizer.bos_token_id] + input_ids\n",
        "  input_ids = torch.tensor(input_ids).cuda().unsqueeze(0)\n",
        "  input_len = input_ids.shape[1]\n",
        "  # We need to pass this because of a bug in with PeftModel\n",
        "  # In a regular HF model this is not required\n",
        "  attention_mask = torch.ones((1, input_len)).cuda()\n",
        "  output = model.generate(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask,\n",
        "      max_new_tokens=model_max_length,\n",
        "      # We are sampling from the models outputs, so try and rerun the\n",
        "      # prompts to see the variation in the outputs.\n",
        "      do_sample=True,\n",
        "      # This is to remove a warning where during generation\n",
        "      # we replace the pad_token_id by eos to stop if the\n",
        "      # model also generates the padding token.\n",
        "      pad_token_id=tokenizer.eos_token_id,\n",
        "  )\n",
        "  # Remove the first tokens as they are the input\n",
        "  output_no_prompt = output[:, input_len:]\n",
        "  return tokenizer.batch_decode(output_no_prompt, skip_special_tokens=True)[0]"
      ],
      "metadata": {
        "id": "qMNfGSeFyNUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_instruction(instruction, model):\n",
        "  print(\"-\" * 100)\n",
        "  print(\"Instruction:\")\n",
        "  print(instruction)\n",
        "  output = run_instruction(instruction, model)\n",
        "  print(\"Answer:\")\n",
        "  print(output)\n",
        "  print()\n",
        "\n",
        "print_instruction(\"What are important concepts in Deep Learning?\", model)\n",
        "print_instruction(\"What is a Large Language Model?\", model)\n",
        "print_instruction(\"What is the capital of Portugal?\", model)"
      ],
      "metadata": {
        "id": "kGxr4fgznpIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train,\n",
        ")\n",
        "trainer.train()\n",
        "# Clear extra memory from optimizer/batches\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "wTDhk9rtURU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load old model so that we can compare outputs\n",
        "pretrained_model = AutoModelForCausalLM.from_pretrained(base_model_name, device_map=\"cuda\")\n",
        "pretrained_model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "finetuned_model = trainer.model"
      ],
      "metadata": {
        "id": "kfcljt_x7gnI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_models(instruction):\n",
        "  print(\"-\" * 100)\n",
        "  print(\"Prompt:\", instruction)\n",
        "  old_output = run_instruction(instruction, pretrained_model)\n",
        "  print(\"Pretrained Answer\", \"-\" * 80)\n",
        "  print(old_output)\n",
        "  new_output = run_instruction(instruction, finetuned_model)\n",
        "  print(\"Instruction tuned Answer\", \"-\" * 80)\n",
        "  print(new_output)\n",
        "  print()\n",
        "\n",
        "compare_models(\"What are important concepts in Deep Learning?\")\n",
        "compare_models(\"What is a Large Language Model?\")\n",
        "compare_models(\"What is the capital of Portugal?\")"
      ],
      "metadata": {
        "id": "2Okb0HREQ7it"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PFEGfIMaBsqG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}